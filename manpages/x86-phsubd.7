.nh
.TH "X86-PHSUBW-PHSUBD" "7" "May 2019" "TTMO" "Intel x86-64 ISA Manual"
.SH NAME
PHSUBW-PHSUBD - PACKED HORIZONTAL SUBTRACT
.TS
allbox;
l l l l l 
l l l l l .
\fB\fCOpcode/Instruction\fR	\fB\fCOp/En\fR	\fB\fC64/32 bit Mode Support\fR	\fB\fCCPUID Feature Flag\fR	\fB\fCDescription\fR
T{
NP 0F 38 05 /r1 PHSUBW mm1, mm2/m64
T}
	RM	V/V	SSSE3	T{
Subtract 16\-bit signed integers horizontally, pack to mm1.
T}
T{
66 0F 38 05 /r PHSUBW xmm1, xmm2/m128
T}
	RM	V/V	SSSE3	T{
Subtract 16\-bit signed integers horizontally, pack to xmm1.
T}
T{
NP 0F 38 06 /r PHSUBD mm1, mm2/m64
T}
	RM	V/V	SSSE3	T{
Subtract 32\-bit signed integers horizontally, pack to mm1.
T}
T{
66 0F 38 06 /r PHSUBD xmm1, xmm2/m128
T}
	RM	V/V	SSSE3	T{
Subtract 32\-bit signed integers horizontally, pack to xmm1.
T}
T{
VEX.128.66.0F38.WIG 05 /r VPHSUBW xmm1, xmm2, xmm3/m128
T}
	RVM	V/V	AVX	T{
Subtract 16\-bit signed integers horizontally, pack to xmm1.
T}
T{
VEX.128.66.0F38.WIG 06 /r VPHSUBD xmm1, xmm2, xmm3/m128
T}
	RVM	V/V	AVX	T{
Subtract 32\-bit signed integers horizontally, pack to xmm1.
T}
T{
VEX.256.66.0F38.WIG 05 /r VPHSUBW ymm1, ymm2, ymm3/m256
T}
	RVM	V/V	AVX2	T{
Subtract 16\-bit signed integers horizontally, pack to ymm1.
T}
T{
VEX.256.66.0F38.WIG 06 /r VPHSUBD ymm1, ymm2, ymm3/m256
T}
	RVM	V/V	AVX2	T{
Subtract 32\-bit signed integers horizontally, pack to ymm1.
T}
.TE

.PP
.RS

.PP
1\&. See note in Section 2.4, “AVX and SSE Instruction Exception
Specification” in the Intel® 64 and IA\-32 Architectures Software
Developer’s Manual, Volume 3A.

.RE

.SH INSTRUCTION OPERAND ENCODING
.TS
allbox;
l l l l l 
l l l l l .
Op/En	Operand 1	Operand 2	Operand 3	Operand 4
RM	ModRM:reg (r, w)	ModRM:r/m (r)	NA	NA
RVM	ModRM:reg (r, w)	VEX.vvvv (r)	ModRM:r/m (r)	NA
.TE

.SH DESCRIPTION
.PP
(V)PHSUBW performs horizontal subtraction on each adjacent pair of
16\-bit signed integers by subtracting the most significant word from the
least significant word of each pair in the source and destination
operands, and packs the signed 16\-bit results to the destination operand
(first operand). (V)PHSUBD performs horizontal subtraction on each
adjacent pair of 32\-bit signed integers by subtracting the most
significant doubleword from the least significant doubleword of each
pair, and packs the signed 32\-bit result to the destination operand.
When the source operand is a 128\-bit memory operand, the operand must be
aligned on a 16\-byte boundary or a general\-protection exception (#GP)
will be generated.

.PP
Legacy SSE version: Both operands can be MMX registers. The second
source operand can be an MMX register or a 64\-bit memory location.

.PP
128\-bit Legacy SSE version: The first source and destination operands
are XMM registers. The second source operand is an XMM register or a
128\-bit memory location. Bits (MAXVL\-1:128) of the corresponding YMM
destination register remain unchanged.

.PP
In 64\-bit mode, use the REX prefix to access additional registers.

.PP
VEX.128 encoded version: The first source and destination operands are
XMM registers. The second source operand is an XMM register or a 128\-bit
memory location. Bits (MAXVL\-1:128) of the destination YMM register are
zeroed.

.PP
VEX.256 encoded version: The first source and destination operands are
YMM registers. The second source operand can be an YMM register or a
256\-bit memory location.

.PP
Note: VEX.L must be 0, otherwise the instruction will #UD.

.SH OPERATION
.SS PHSUBW (with 64\-bit operands)
.PP
.RS

.nf
mm1[15\-0] = mm1[15\-0] \- mm1[31\-16];
mm1[31\-16] = mm1[47\-32] \- mm1[63\-48];
mm1[47\-32] = mm2/m64[15\-0] \- mm2/m64[31\-16];
mm1[63\-48] = mm2/m64[47\-32] \- mm2/m64[63\-48];

.fi
.RE

.SS PHSUBW (with 128\-bit operands)
.PP
.RS

.nf
xmm1[15\-0] = xmm1[15\-0] \- xmm1[31\-16];
xmm1[31\-16] = xmm1[47\-32] \- xmm1[63\-48];
xmm1[47\-32] = xmm1[79\-64] \- xmm1[95\-80];
xmm1[63\-48] = xmm1[111\-96] \- xmm1[127\-112];
xmm1[79\-64] = xmm2/m128[15\-0] \- xmm2/m128[31\-16];
xmm1[95\-80] = xmm2/m128[47\-32] \- xmm2/m128[63\-48];
xmm1[111\-96] = xmm2/m128[79\-64] \- xmm2/m128[95\-80];
xmm1[127\-112] = xmm2/m128[111\-96] \- xmm2/m128[127\-112];

.fi
.RE

.SS VPHSUBW (VEX.128 encoded version)
.PP
.RS

.nf
DEST[15:0]←SRC1[15:0] \- SRC1[31:16]
DEST[31:16]←SRC1[47:32] \- SRC1[63:48]
DEST[47:32]←SRC1[79:64] \- SRC1[95:80]
DEST[63:48]←SRC1[111:96] \- SRC1[127:112]
DEST[79:64]←SRC2[15:0] \- SRC2[31:16]
DEST[95:80]←SRC2[47:32] \- SRC2[63:48]
DEST[111:96]←SRC2[79:64] \- SRC2[95:80]
DEST[127:112]←SRC2[111:96] \- SRC2[127:112]
DEST[MAXVL\-1:128] ← 0

.fi
.RE

.SS VPHSUBW (VEX.256 encoded version)
.PP
.RS

.nf
DEST[15:0]←SRC1[15:0] \- SRC1[31:16]
DEST[31:16]←SRC1[47:32] \- SRC1[63:48]
DEST[47:32]←SRC1[79:64] \- SRC1[95:80]
DEST[63:48]←SRC1[111:96] \- SRC1[127:112]
DEST[79:64]←SRC2[15:0] \- SRC2[31:16]
DEST[95:80]←SRC2[47:32] \- SRC2[63:48]
DEST[111:96]←SRC2[79:64] \- SRC2[95:80]
DEST[127:112]←SRC2[111:96] \- SRC2[127:112]
DEST[143:128]←SRC1[143:128] \- SRC1[159:144]
DEST[159:144]←SRC1[175:160] \- SRC1[191:176]
DEST[175:160]←SRC1[207:192] \- SRC1[223:208]
DEST[191:176]←SRC1[239:224] \- SRC1[255:240]
DEST[207:192]←SRC2[143:128] \- SRC2[159:144]
DEST[223:208]←SRC2[175:160] \- SRC2[191:176]
DEST[239:224]←SRC2[207:192] \- SRC2[223:208]
DEST[255:240]←SRC2[239:224] \- SRC2[255:240]

.fi
.RE

.SS PHSUBD (with 64\-bit operands)
.PP
.RS

.nf
mm1[31\-0] = mm1[31\-0] \- mm1[63\-32];
mm1[63\-32] = mm2/m64[31\-0] \- mm2/m64[63\-32];

.fi
.RE

.SS PHSUBD (with 128\-bit operands)
.PP
.RS

.nf
xmm1[31\-0] = xmm1[31\-0] \- xmm1[63\-32];
xmm1[63\-32] = xmm1[95\-64] \- xmm1[127\-96];
xmm1[95\-64] = xmm2/m128[31\-0] \- xmm2/m128[63\-32];
xmm1[127\-96] = xmm2/m128[95\-64] \- xmm2/m128[127\-96];

.fi
.RE

.SS VPHSUBD (VEX.128 encoded version)
.PP
.RS

.nf
DEST[31\-0]←SRC1[31\-0] \- SRC1[63\-32]
DEST[63\-32]←SRC1[95\-64] \- SRC1[127\-96]
DEST[95\-64]←SRC2[31\-0] \- SRC2[63\-32]
DEST[127\-96]←SRC2[95\-64] \- SRC2[127\-96]
DEST[MAXVL\-1:128] ← 0

.fi
.RE

.SS VPHSUBD (VEX.256 encoded version)
.PP
.RS

.nf
DEST[31:0]←SRC1[31:0] \- SRC1[63:32]
DEST[63:32]←SRC1[95:64] \- SRC1[127:96]
DEST[95:64]←SRC2[31:0] \- SRC2[63:32]
DEST[127:96]←SRC2[95:64] \- SRC2[127:96]
DEST[159:128]←SRC1[159:128] \- SRC1[191:160]
DEST[191:160]←SRC1[223:192] \- SRC1[255:224]
DEST[223:192]←SRC2[159:128] \- SRC2[191:160]
DEST[255:224]←SRC2[223:192] \- SRC2[255:224]

.fi
.RE

.SH INTEL C/C++ COMPILER INTRINSIC EQUIVALENTS
.PP
.RS

.nf
PHSUBW: \_\_m64 \_mm\_hsub\_pi16 (\_\_m64 a, \_\_m64 b)

PHSUBD: \_\_m64 \_mm\_hsub\_pi32 (\_\_m64 a, \_\_m64 b)

(V)PHSUBW: \_\_m128i \_mm\_hsub\_epi16 (\_\_m128i a, \_\_m128i b)

(V)PHSUBD: \_\_m128i \_mm\_hsub\_epi32 (\_\_m128i a, \_\_m128i b)

VPHSUBW: \_\_m256i \_mm256\_hsub\_epi16 (\_\_m256i a, \_\_m256i b)

VPHSUBD: \_\_m256i \_mm256\_hsub\_epi32 (\_\_m256i a, \_\_m256i b)

.fi
.RE

.SH SIMD FLOATING\-POINT EXCEPTIONS
.PP
None.

.SH OTHER EXCEPTIONS
.PP
See Exceptions Type 4; additionally

.TS
allbox;
l l 
l l .
#UD	If VEX.L = 1.
.TE

.SH SEE ALSO
.PP
x86\-manpages(7) for a list of other x86\-64 man pages.

.SH COLOPHON
.PP
This UNOFFICIAL, mechanically\-separated, non\-verified reference is
provided for convenience, but it may be incomplete or broken in
various obvious or non\-obvious ways. Refer to Intel® 64 and IA\-32
Architectures Software Developer’s Manual for anything serious.

.br
This page is generated by scripts; therefore may contain visual or semantical bugs. Please report them (or better, fix them) on https://github.com/ttmo-O/x86-manpages.

.br
MIT licensed by TTMO 2020 (Turkish Unofficial Chamber of Reverse Engineers - https://ttmo.re).
