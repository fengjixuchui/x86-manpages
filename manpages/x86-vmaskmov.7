.nh
.TH "X86-VMASKMOV" "7" "May 2019" "TTMO" "Intel x86-64 ISA Manual"
.SH NAME
VMASKMOV - CONDITIONAL SIMD PACKED LOADS AND STORES
.TS
allbox;
l l l l l 
l l l l l .
\fB\fCOpcode/Instruction\fR	\fB\fCOp/En\fR	\fB\fC64/32\-bit Mode\fR	\fB\fCCPUID Feature Flag\fR	\fB\fCDescription\fR
T{
VEX.128.66.0F38.W0 2C /r VMASKMOVPS xmm1, xmm2, m128
T}
	RVM	V/V	AVX	T{
Conditionally load packed single\-precision values from xmm1.
T}
T{
VEX.256.66.0F38.W0 2C /r VMASKMOVPS ymm1, ymm2, m256
T}
	RVM	V/V	AVX	T{
Conditionally load packed single\-precision values from ymm1.
T}
T{
VEX.128.66.0F38.W0 2D /r VMASKMOVPD xmm1, xmm2, m128
T}
	RVM	V/V	AVX	T{
Conditionally load packed double\-precision values from xmm1.
T}
T{
VEX.256.66.0F38.W0 2D /r VMASKMOVPD ymm1, ymm2, m256
T}
	RVM	V/V	AVX	T{
Conditionally load packed double\-precision values from ymm1.
T}
T{
VEX.128.66.0F38.W0 2E /r VMASKMOVPS m128, xmm1, xmm2
T}
	MVR	V/V	AVX	T{
Conditionally store packed single\-precision values from xmm1.
T}
T{
VEX.256.66.0F38.W0 2E /r VMASKMOVPS m256, ymm1, ymm2
T}
	MVR	V/V	AVX	T{
Conditionally store packed single\-precision values from ymm1.
T}
T{
VEX.128.66.0F38.W0 2F /r VMASKMOVPD m128, xmm1, xmm2
T}
	MVR	V/V	AVX	T{
Conditionally store packed double\-precision values from xmm1.
T}
T{
VEX.256.66.0F38.W0 2F /r VMASKMOVPD m256, ymm1, ymm2
T}
	MVR	V/V	AVX	T{
Conditionally store packed double\-precision values from ymm1.
T}
.TE

.SH INSTRUCTION OPERAND ENCODING
.TS
allbox;
l l l l l 
l l l l l .
Op/En	Operand 1	Operand 2	Operand 3	Operand 4
RVM	ModRM:reg (w)	VEX.vvvv (r)	ModRM:r/m (r)	NA
MVR	ModRM:r/m (w)	VEX.vvvv (r)	ModRM:reg (r)	NA
.TE

.SH DESCRIPTION
.PP
Conditionally moves packed data elements from the second source operand
into the corresponding data element of the destination operand,
depending on the mask bits associated with each data element. The mask
bits are specified in the first source operand.

.PP
The mask bit for each data element is the most significant bit of that
element in the first source operand. If a mask is 1, the corresponding
data element is copied from the second source operand to the destination
operand. If the mask is 0, the corresponding data element is set to zero
in the load form of these instructions, and unmodified in the store
form.

.PP
The second source operand is a memory address for the load form of these
instruction. The destination operand is a memory address for the store
form of these instructions. The other operands are both XMM registers
(for VEX.128 version) or YMM registers (for VEX.256 version).

.PP
Faults occur only due to mask\-bit required memory accesses that caused
the faults. Faults will not occur due to referencing any memory location
if the corresponding mask bit for that memory location is 0. For
example, no faults will be detected if the mask bits are all zero.

.PP
Unlike previous MASKMOV instructions (MASKMOVQ and MASKMOVDQU), a
nontemporal hint is not applied to these instructions.

.PP
Instruction behavior on alignment check reporting with mask bits of less
than all 1s are the same as with mask bits of all 1s.

.PP
VMASKMOV should not be used to access memory mapped I/O and un\-cached
memory as the access and the ordering of the individual loads or stores
it does is implementation specific.

.PP
In cases where mask bits indicate data should not be loaded or stored
paging A and D bits will be set in an implementation dependent way.
However, A and D bits are always set for pages where data is actually
loaded/stored.

.PP
Note: for load forms, the first source (the mask) is encoded in
VEX.vvvv; the second source is encoded in rm\_field, and the destination
register is encoded in reg\_field.

.PP
Note: for store forms, the first source (the mask) is encoded in
VEX.vvvv; the second source register is encoded in reg\_field, and the
destination memory location is encoded in rm\_field.

.SH OPERATION
.SS VMASKMOVPS \-128\-bit load
.PP
.RS

.nf
DEST[31:0]←IF (SRC1[31]) Load\_32(mem) ELSE 0
DEST[63:32]←IF (SRC1[63]) Load\_32(mem + 4) ELSE 0
DEST[95:64]←IF (SRC1[95]) Load\_32(mem + 8) ELSE 0
DEST[127:97]←IF (SRC1[127]) Load\_32(mem + 12) ELSE 0
DEST[MAXVL\-1:128] ← 0

.fi
.RE

.SS VMASKMOVPS \- 256\-bit load
.PP
.RS

.nf
DEST[31:0]←IF (SRC1[31]) Load\_32(mem) ELSE 0
DEST[63:32]←IF (SRC1[63]) Load\_32(mem + 4) ELSE 0
DEST[95:64]←IF (SRC1[95]) Load\_32(mem + 8) ELSE 0
DEST[127:96]←IF (SRC1[127]) Load\_32(mem + 12) ELSE 0
DEST[159:128]←IF (SRC1[159]) Load\_32(mem + 16) ELSE 0
DEST[191:160]←IF (SRC1[191]) Load\_32(mem + 20) ELSE 0
DEST[223:192]←IF (SRC1[223]) Load\_32(mem + 24) ELSE 0
DEST[255:224]←IF (SRC1[255]) Load\_32(mem + 28) ELSE 0

.fi
.RE

.SS VMASKMOVPD \- 128\-bit load
.PP
.RS

.nf
DEST[63:0]←IF (SRC1[63]) Load\_64(mem) ELSE 0
DEST[127:64]←IF (SRC1[127]) Load\_64(mem + 16) ELSE 0
DEST[MAXVL\-1:128] ← 0

.fi
.RE

.SS VMASKMOVPD \- 256\-bit load
.PP
.RS

.nf
DEST[63:0]←IF (SRC1[63]) Load\_64(mem) ELSE 0
DEST[127:64]←IF (SRC1[127]) Load\_64(mem + 8) ELSE 0
DEST[195:128]←IF (SRC1[191]) Load\_64(mem + 16) ELSE 0
DEST[255:196]←IF (SRC1[255]) Load\_64(mem + 24) ELSE 0

.fi
.RE

.SS VMASKMOVPS \- 128\-bit store
.PP
.RS

.nf
IF (SRC1[31]) DEST[31:0]←SRC2[31:0]
IF (SRC1[63]) DEST[63:32]←SRC2[63:32]
IF (SRC1[95]) DEST[95:64]←SRC2[95:64]
IF (SRC1[127]) DEST[127:96]←SRC2[127:96]

.fi
.RE

.SS VMASKMOVPS \- 256\-bit store
.PP
.RS

.nf
IF (SRC1[31]) DEST[31:0]←SRC2[31:0]
IF (SRC1[63]) DEST[63:32]←SRC2[63:32]
IF (SRC1[95]) DEST[95:64]←SRC2[95:64]
IF (SRC1[127]) DEST[127:96]←SRC2[127:96]
IF (SRC1[159]) DEST[159:128]←SRC2[159:128]
IF (SRC1[191]) DEST[191:160]←SRC2[191:160]
IF (SRC1[223]) DEST[223:192]←SRC2[223:192]
IF (SRC1[255]) DEST[255:224]←SRC2[255:224]

.fi
.RE

.SS VMASKMOVPD \- 128\-bit store
.PP
.RS

.nf
IF (SRC1[63]) DEST[63:0]←SRC2[63:0]
IF (SRC1[127]) DEST[127:64]←SRC2[127:64]

.fi
.RE

.SS VMASKMOVPD \- 256\-bit store
.PP
.RS

.nf
IF (SRC1[63]) DEST[63:0]←SRC2[63:0]
IF (SRC1[127]) DEST[127:64]←SRC2[127:64]
IF (SRC1[191]) DEST[191:128]←SRC2[191:128]
IF (SRC1[255]) DEST[255:192]←SRC2[255:192]

.fi
.RE

.SH INTEL C/C++ COMPILER INTRINSIC EQUIVALENT
.PP
.RS

.nf
\_\_m256 \_mm256\_maskload\_ps(float const *a, \_\_m256i mask)

void \_mm256\_maskstore\_ps(float *a, \_\_m256i mask, \_\_m256 b)

\_\_m256d \_mm256\_maskload\_pd(double *a, \_\_m256i mask);

void \_mm256\_maskstore\_pd(double *a, \_\_m256i mask, \_\_m256d b);

\_\_m128 \_mm\_maskload\_ps(float const *a, \_\_m128i mask)

void \_mm\_maskstore\_ps(float *a, \_\_m128i mask, \_\_m128 b)

\_\_m128d \_mm\_maskload\_pd(double const *a, \_\_m128i mask);

void \_mm\_maskstore\_pd(double *a, \_\_m128i mask, \_\_m128d b);

.fi
.RE

.SH SIMD FLOATING\-POINT EXCEPTIONS
.PP
None

.SH OTHER EXCEPTIONS
.PP
See Exceptions Type 6 (No AC# reported for any mask bit combinations);

.PP
additionally

.TS
allbox;
l l 
l l .
#UD	If VEX.W = 1.
.TE

.SH SEE ALSO
.PP
x86\-manpages(7) for a list of other x86\-64 man pages.

.SH COLOPHON
.PP
This UNOFFICIAL, mechanically\-separated, non\-verified reference is
provided for convenience, but it may be incomplete or broken in
various obvious or non\-obvious ways. Refer to Intel® 64 and IA\-32
Architectures Software Developer’s Manual for anything serious.

.br
This page is generated by scripts; therefore may contain visual or semantical bugs. Please report them (or better, fix them) on https://github.com/ttmo-O/x86-manpages.

.br
MIT licensed by TTMO 2020 (Turkish Unofficial Chamber of Reverse Engineers - https://ttmo.re).
