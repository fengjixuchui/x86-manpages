.nh
.TH "X86-VPCMPB-VPCMPUB" "7" "May 2019" "TTMO" "Intel x86-64 ISA Manual"
.SH NAME
VPCMPB-VPCMPUB - COMPARE PACKED BYTE VALUES INTO MASK
.TS
allbox;
l l l l l 
l l l l l .
\fB\fCOpcode/Instruction\fR	\fB\fCOp/En\fR	\fB\fC64/32 bit Mode Support\fR	\fB\fCCPUID Feature Flag\fR	\fB\fCDescription\fR
T{
EVEX.128.66.0F3A.W0 3F /r ib VPCMPB k1 {k2}, xmm2, xmm3/m128, imm8
T}
	A	V/V	AVX512VL AVX512BW	T{
Compare packed signed byte values in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
T}
T{
EVEX.256.66.0F3A.W0 3F /r ib VPCMPB k1 {k2}, ymm2, ymm3/m256, imm8
T}
	A	V/V	AVX512VL AVX512BW	T{
Compare packed signed byte values in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
T}
T{
EVEX.512.66.0F3A.W0 3F /r ib VPCMPB k1 {k2}, zmm2, zmm3/m512, imm8
T}
	A	V/V	AVX512BW	T{
Compare packed signed byte values in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
T}
T{
EVEX.128.66.0F3A.W0 3E /r ib VPCMPUB k1 {k2}, xmm2, xmm3/m128, imm8
T}
	A	V/V	AVX512VL AVX512BW	T{
Compare packed unsigned byte values in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
T}
T{
EVEX.256.66.0F3A.W0 3E /r ib VPCMPUB k1 {k2}, ymm2, ymm3/m256, imm8
T}
	A	V/V	AVX512VL AVX512BW	T{
Compare packed unsigned byte values in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
T}
T{
EVEX.512.66.0F3A.W0 3E /r ib VPCMPUB k1 {k2}, zmm2, zmm3/m512, imm8
T}
	A	V/V	AVX512BW	T{
Compare packed unsigned byte values in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
T}
.TE

.SH INSTRUCTION OPERAND ENCODING
.TS
allbox;
l l l l l l 
l l l l l l .
Op/En	Tuple Type	Operand 1	Operand 2	Operand 3	Operand 4
A	Full Mem	ModRM:reg (w)	vvvv (r)	ModRM:r/m (r)	NA
.TE

.SS Description
.PP
Performs a SIMD compare of the packed byte values in the second source
operand and the first source operand and returns the results of the
comparison to the mask destination operand. The comparison predicate
operand (immediate byte) specifies the type of comparison performed on
each pair of packed values in the two source operands. The result of
each comparison is a single mask bit result of 1 (comparison true) or 0
(comparison false).

.PP
VPCMPB performs a comparison between pairs of signed byte values.

.PP
VPCMPUB performs a comparison between pairs of unsigned byte values.

.PP
The first source operand (second operand) is a ZMM/YMM/XMM register. The
second source operand can be a ZMM/YMM/XMM register or a 512/256/128\-bit
memory location. The destination operand (first operand) is a mask
register k1. Up to 64/32/16 comparisons are performed with results
written to the destination operand under the writemask k2.

.PP
The comparison predicate operand is an 8\-bit immediate: bits 2:0 define
the type of comparison to be performed. Bits 3 through 7 of the
immediate are reserved. Compiler can implement the pseudo\-op mnemonic
listed in Table 5\-17.

.TS
allbox;
l l 
l l .
\fB\fCPseudo\-Op\fR	\fB\fCPCMPM Implementation\fR
VPCMPEQ* reg1, reg2, reg3	VPCMP
*
 reg1, reg2, reg3, 0
VPCMPLT* reg1, reg2, reg3	VPCMP
*
reg1, reg2, reg3, 1
VPCMPLE* reg1, reg2, reg3	VPCMP
*
 reg1, reg2, reg3, 2
VPCMPNEQ* reg1, reg2, reg3	VPCMP
*
 reg1, reg2, reg3, 4
VPPCMPNLT* reg1, reg2, reg3	VPCMP
*
 reg1, reg2, reg3, 5
VPCMPNLE* reg1, reg2, reg3	VPCMP
*
 reg1, reg2, reg3, 6
.TE

.PP
Table 5\-17. Pseudo\-Op and VPCMP* Implementation

.SS Operation
.PP
.RS

.nf
CASE (COMPARISON PREDICATE) OF
    0: OP←EQ;
    1: OP←LT;
    2: OP←LE;
    3: OP←FALSE;
    4: OP←NEQ;
    5: OP←NLT;
    6: OP←NLE;
    7: OP←TRUE;
ESAC;

.fi
.RE

.SS VPCMPB (EVEX encoded versions)
.PP
.RS

.nf
(KL, VL) = (16, 128), (32, 256), (64, 512)
FOR j←0 TO KL\-1
    i←j * 8
    IF k2[j] OR *no writemask*
        THEN
            CMP←SRC1[i+7:i] OP SRC2[i+7:i];
            IF CMP = TRUE
                THEN DEST[j]←1;
                ELSE DEST[j]←0; FI;
        ELSE DEST[j] = 0
                    ; zeroing\-masking onlyFI;
    FI;
ENDFOR
DEST[MAX\_KL\-1:KL] ← 0

.fi
.RE

.SS VPCMPUB (EVEX encoded versions)
.PP
.RS

.nf
(KL, VL) = (16, 128), (32, 256), (64, 512)
FOR j←0 TO KL\-1
    i←j * 8
    IF k2[j] OR *no writemask*
        THEN
            CMP←SRC1[i+7:i] OP SRC2[i+7:i];
            IF CMP = TRUE
                THEN DEST[j]←1;
                ELSE DEST[j]←0; FI;
        ELSE DEST[j] = 0
                    ; zeroing\-masking onlyFI;
    FI;
ENDFOR
DEST[MAX\_KL\-1:KL] ← 0

.fi
.RE

.SS Intel C/C++ Compiler Intrinsic Equivalent
.PP
.RS

.nf
VPCMPB \_\_mmask64 \_mm512\_cmp\_epi8\_mask( \_\_m512i a, \_\_m512i b, int cmp);

VPCMPB \_\_mmask64 \_mm512\_mask\_cmp\_epi8\_mask( \_\_mmask64 m, \_\_m512i a, \_\_m512i b, int cmp);

VPCMPB \_\_mmask32 \_mm256\_cmp\_epi8\_mask( \_\_m256i a, \_\_m256i b, int cmp);

VPCMPB \_\_mmask32 \_mm256\_mask\_cmp\_epi8\_mask( \_\_mmask32 m, \_\_m256i a, \_\_m256i b, int cmp);

VPCMPB \_\_mmask16 \_mm\_cmp\_epi8\_mask( \_\_m128i a, \_\_m128i b, int cmp);

VPCMPB \_\_mmask16 \_mm\_mask\_cmp\_epi8\_mask( \_\_mmask16 m, \_\_m128i a, \_\_m128i b, int cmp);

VPCMPB \_\_mmask64 \_mm512\_cmp[eq|ge|gt|le|lt|neq]\_epi8\_mask( \_\_m512i a, \_\_m512i b);

VPCMPB \_\_mmask64 \_mm512\_mask\_cmp[eq|ge|gt|le|lt|neq]\_epi8\_mask( \_\_mmask64 m, \_\_m512i a, \_\_m512i b);

VPCMPB \_\_mmask32 \_mm256\_cmp[eq|ge|gt|le|lt|neq]\_epi8\_mask( \_\_m256i a, \_\_m256i b);

VPCMPB \_\_mmask32 \_mm256\_mask\_cmp[eq|ge|gt|le|lt|neq]\_epi8\_mask( \_\_mmask32 m, \_\_m256i a, \_\_m256i b);

VPCMPB \_\_mmask16 \_mm\_cmp[eq|ge|gt|le|lt|neq]\_epi8\_mask( \_\_m128i a, \_\_m128i b);

VPCMPB \_\_mmask16 \_mm\_mask\_cmp[eq|ge|gt|le|lt|neq]\_epi8\_mask( \_\_mmask16 m, \_\_m128i a, \_\_m128i b);

VPCMPUB \_\_mmask64 \_mm512\_cmp\_epu8\_mask( \_\_m512i a, \_\_m512i b, int cmp);

VPCMPUB \_\_mmask64 \_mm512\_mask\_cmp\_epu8\_mask( \_\_mmask64 m, \_\_m512i a, \_\_m512i b, int cmp);

VPCMPUB \_\_mmask32 \_mm256\_cmp\_epu8\_mask( \_\_m256i a, \_\_m256i b, int cmp);

VPCMPUB \_\_mmask32 \_mm256\_mask\_cmp\_epu8\_mask( \_\_mmask32 m, \_\_m256i a, \_\_m256i b, int cmp);

VPCMPUB \_\_mmask16 \_mm\_cmp\_epu8\_mask( \_\_m128i a, \_\_m128i b, int cmp);

VPCMPUB \_\_mmask16 \_mm\_mask\_cmp\_epu8\_mask( \_\_mmask16 m, \_\_m128i a, \_\_m128i b, int cmp);

VPCMPUB \_\_mmask64 \_mm512\_cmp[eq|ge|gt|le|lt|neq]\_epu8\_mask( \_\_m512i a, \_\_m512i b, int cmp);

VPCMPUB \_\_mmask64 \_mm512\_mask\_cmp[eq|ge|gt|le|lt|neq]\_epu8\_mask( \_\_mmask64 m, \_\_m512i a, \_\_m512i b, int cmp);

VPCMPUB \_\_mmask32 \_mm256\_cmp[eq|ge|gt|le|lt|neq]\_epu8\_mask( \_\_m256i a, \_\_m256i b, int cmp);

VPCMPUB \_\_mmask32 \_mm256\_mask\_cmp[eq|ge|gt|le|lt|neq]\_epu8\_mask( \_\_mmask32 m, \_\_m256i a, \_\_m256i b, int cmp);

VPCMPUB \_\_mmask16 \_mm\_cmp[eq|ge|gt|le|lt|neq]\_epu8\_mask( \_\_m128i a, \_\_m128i b, int cmp);

VPCMPUB \_\_mmask16 \_mm\_mask\_cmp[eq|ge|gt|le|lt|neq]\_epu8\_mask( \_\_mmask16 m, \_\_m128i a, \_\_m128i b, int cmp);

.fi
.RE

.SS SIMD Floating\-Point Exceptions
.PP
None

.SS Other Exceptions
.PP
EVEX\-encoded instruction, see Exceptions Type E4.nb.

.SH SEE ALSO
.PP
x86\-manpages(7) for a list of other x86\-64 man pages.

.SH COLOPHON
.PP
This UNOFFICIAL, mechanically\-separated, non\-verified reference is
provided for convenience, but it may be incomplete or broken in
various obvious or non\-obvious ways. Refer to Intel® 64 and IA\-32
Architectures Software Developer’s Manual for anything serious.

.br
This page is generated by scripts; therefore may contain visual or semantical bugs. Please report them (or better, fix them) on https://github.com/ttmo-O/x86-manpages.

.br
MIT licensed by TTMO 2020 (Turkish Unofficial Chamber of Reverse Engineers - https://ttmo.re).
