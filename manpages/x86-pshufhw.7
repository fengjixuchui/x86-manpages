.nh
.TH "X86-PSHUFHW" "7" "May 2019" "TTMO" "Intel x86-64 ISA Manual"
.SH NAME
PSHUFHW - SHUFFLE PACKED HIGH WORDS
.TS
allbox;
l l l l l 
l l l l l .
\fB\fCOpcode/Instruction\fR	\fB\fCOp/En\fR	\fB\fC64/32 bit Mode Support\fR	\fB\fCCPUID Feature Flag\fR	\fB\fCDescription\fR
F3 0F 70 /imm8	A	V/V	SSE2	T{
Shuffle the high words in xmm1.
T}
T{
VEX.128.F3.0F.WIG 70 /r ib VPSHUFHW xmm1, xmm2/m128, imm8
T}
	A	V/V	AVX	T{
Shuffle the high words in xmm1.
T}
T{
VEX.256.F3.0F.WIG 70 /r ib VPSHUFHW ymm1, ymm2/m256, imm8
T}
	A	V/V	AVX2	T{
Shuffle the high words in ymm1.
T}
T{
EVEX.128.F3.0F.WIG 70 /r ib VPSHUFHW xmm1 {k1}{z}, xmm2/m128, imm8
T}
	B	V/V	AVX512VL AVX512BW	T{
Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1 under write mask k1.
T}
T{
EVEX.256.F3.0F.WIG 70 /r ib VPSHUFHW ymm1 {k1}{z}, ymm2/m256, imm8
T}
	B	V/V	AVX512VL AVX512BW	T{
Shuffle the high words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1 under write mask k1.
T}
T{
EVEX.512.F3.0F.WIG 70 /r ib VPSHUFHW zmm1 {k1}{z}, zmm2/m512, imm8
T}
	B	V/V	AVX512BW	T{
Shuffle the high words in zmm2/m512 based on the encoding in imm8 and store the result in zmm1 under write mask k1.
T}
.TE

.SH INSTRUCTION OPERAND ENCODING
.TS
allbox;
l l l l l l 
l l l l l l .
Op/En	Tuple Type	Operand 1	Operand 2	Operand 3	Operand 4
A	NA	ModRM:reg (w)	ModRM:r/m (r)	imm8	NA
B	Full Mem	ModRM:reg (w)	ModRM:r/m (r)	Imm8	NA
.TE

.SH DESCRIPTION
.PP
Copies words from the high quadword of a 128\-bit lane of the source
operand and inserts them in the high quadword of the destination operand
at word locations (of the respective lane) selected with the immediate
operand. This 256\-bit operation is similar to the in\-lane operation used
by the 256\-bit VPSHUFD instruction, which is illustrated in Figure 4\-16\&. For 128\-\&bit operation, only the
low 128\-\&bit lane is operative. Each 2\-\&bit field in the immediate operand
selects the contents of one word location in the high quadword of the
destination operand. The binary encodings of the immediate operand
fields select words (0, 1, 2 or 3, 4) from the high quadword of the
source operand to be copied to the destination operand. The low quadword
of the source operand is copied to the low quadword of the destination
operand, for each 128\-\&bit lane.

.PP
Note that this instruction permits a word in the high quadword of the
source operand to be copied to more than one word location in the high
quadword of the destination operand.

.PP
In 64\-bit mode and not encoded with VEX/EVEX, using a REX prefix in the
form of REX.R permits this instruction to access additional registers
(XMM8\-XMM15).

.PP
128\-bit Legacy SSE version: The destination operand is an XMM register.
The source operand can be an XMM register or a 128\-bit memory location.
Bits (MAXVL\-1:128) of the corresponding YMM destination register remain
unchanged.

.PP
VEX.128 encoded version: The destination operand is an XMM register. The
source operand can be an XMM register or a 128\-bit memory location. Bits
(MAXVL\-1:128) of the destination YMM register are zeroed. VEX.vvvv is
reserved and must be 1111b, VEX.L must be 0, otherwise the instruction
will #UD.

.PP
VEX.256 encoded version: The destination operand is an YMM register. The
source operand can be an YMM register or a 256\-bit memory location.

.PP
EVEX encoded version: The destination operand is a ZMM/YMM/XMM
registers. The source operand can be a ZMM/YMM/XMM register, a
512/256/128\-bit memory location. The destination is updated according to
the writemask.

.PP
Note: In VEX encoded versions, VEX.vvvv is reserved and must be 1111b
otherwise instructions will #UD.

.SH OPERATION
.SS PSHUFHW (128\-bit Legacy SSE version)
.PP
.RS

.nf
DEST[63:0] ← SRC[63:0]
DEST[79:64]←(SRC >> (imm[1:0] *16))[79:64]
DEST[95:80]←(SRC >> (imm[3:2] * 16))[79:64]
DEST[111:96]←(SRC >> (imm[5:4] * 16))[79:64]
DEST[127:112]←(SRC >> (imm[7:6] * 16))[79:64]
DEST[MAXVL\-1:128] (Unmodified)

.fi
.RE

.SS VPSHUFHW (VEX.128 encoded version)
.PP
.RS

.nf
DEST[63:0] ← SRC1[63:0]
DEST[79:64]←(SRC1 >> (imm[1:0] *16))[79:64]
DEST[95:80]←(SRC1 >> (imm[3:2] * 16))[79:64]
DEST[111:96]←(SRC1 >> (imm[5:4] * 16))[79:64]
DEST[127:112]←(SRC1 >> (imm[7:6] * 16))[79:64]
DEST[MAXVL\-1:128] ← 0

.fi
.RE

.SS VPSHUFHW (VEX.256 encoded version)
.PP
.RS

.nf
DEST[63:0] ← SRC1[63:0]
DEST[79:64]←(SRC1 >> (imm[1:0] *16))[79:64]
DEST[95:80]←(SRC1 >> (imm[3:2] * 16))[79:64]
DEST[111:96]←(SRC1 >> (imm[5:4] * 16))[79:64]
DEST[127:112]←(SRC1 >> (imm[7:6] * 16))[79:64]
DEST[191:128] ← SRC1[191:128]
DEST[207192]←(SRC1 >> (imm[1:0] *16))[207:192]
DEST[223:208]←(SRC1 >> (imm[3:2] * 16))[207:192]
DEST[239:224]←(SRC1 >> (imm[5:4] * 16))[207:192]
DEST[255:240]←(SRC1 >> (imm[7:6] * 16))[207:192]
DEST[MAXVL\-1:256] ← 0

.fi
.RE

.SS VPSHUFHW (EVEX encoded versions)
.PP
.RS

.nf
(KL, VL) = (8, 128), (16, 256), (32, 512)
IF VL >= 128
    TMP\_DEST[63:0] ← SRC1[63:0]
    TMP\_DEST[79:64]←(SRC1 >> (imm[1:0] *16))[79:64]
    TMP\_DEST[95:80]←(SRC1 >> (imm[3:2] * 16))[79:64]
    TMP\_DEST[111:96]←(SRC1 >> (imm[5:4] * 16))[79:64]
    TMP\_DEST[127:112]←(SRC1 >> (imm[7:6] * 16))[79:64]
FI;
IF VL >= 256
    TMP\_DEST[191:128] ← SRC1[191:128]
    TMP\_DEST[207:192]←(SRC1 >> (imm[1:0] *16))[207:192]
    TMP\_DEST[223:208]←(SRC1 >> (imm[3:2] * 16))[207:192]
    TMP\_DEST[239:224]←(SRC1 >> (imm[5:4] * 16))[207:192]
    TMP\_DEST[255:240]←(SRC1 >> (imm[7:6] * 16))[207:192]
FI;
IF VL >= 512
    TMP\_DEST[319:256] ← SRC1[319:256]
    TMP\_DEST[335:320]←(SRC1 >> (imm[1:0] *16))[335:320]
    TMP\_DEST[351:336]←(SRC1 >> (imm[3:2] * 16))[335:320]
    TMP\_DEST[367:352]←(SRC1 >> (imm[5:4] * 16))[335:320]
    TMP\_DEST[383:368]←(SRC1 >> (imm[7:6] * 16))[335:320]
    TMP\_DEST[447:384] ← SRC1[447:384]
    TMP\_DEST[463:448]←(SRC1 >> (imm[1:0] *16))[463:448]
    TMP\_DEST[479:464]←(SRC1 >> (imm[3:2] * 16))[463:448]
    TMP\_DEST[495:480]←(SRC1 >> (imm[5:4] * 16))[463:448]
    TMP\_DEST[511:496]←(SRC1 >> (imm[7:6] * 16))[463:448]
FI;
FOR j←0 TO KL\-1
    i←j * 16
    IF k1[j] OR *no writemask*
        THEN DEST[i+15:i]←TMP\_DEST[i+15:i];
        ELSE
            IF *merging\-masking*
                        ; merging\-masking
                THEN *DEST[i+15:i] remains unchanged*
                ELSE *zeroing\-masking*
                            ; zeroing\-masking
                    DEST[i+15:i] ← 0
            FI
    FI;
ENDFOR
DEST[MAXVL\-1:VL] ← 0

.fi
.RE

.SH INTEL C/C++ COMPILER INTRINSIC EQUIVALENT
.PP
.RS

.nf
VPSHUFHW \_\_m512i \_mm512\_shufflehi\_epi16(\_\_m512i a, int n);

VPSHUFHW \_\_m512i \_mm512\_mask\_shufflehi\_epi16(\_\_m512i s, \_\_mmask16 k, \_\_m512i a, int n );

VPSHUFHW \_\_m512i \_mm512\_maskz\_shufflehi\_epi16( \_\_mmask16 k, \_\_m512i a, int n );

VPSHUFHW \_\_m256i \_mm256\_mask\_shufflehi\_epi16(\_\_m256i s, \_\_mmask8 k, \_\_m256i a, int n );

VPSHUFHW \_\_m256i \_mm256\_maskz\_shufflehi\_epi16( \_\_mmask8 k, \_\_m256i a, int n );

VPSHUFHW \_\_m128i \_mm\_mask\_shufflehi\_epi16(\_\_m128i s, \_\_mmask8 k, \_\_m128i a, int n );

VPSHUFHW \_\_m128i \_mm\_maskz\_shufflehi\_epi16( \_\_mmask8 k, \_\_m128i a, int n );

(V)PSHUFHW:\_\_m128i \_mm\_shufflehi\_epi16(\_\_m128i a, int n)

VPSHUFHW:\_\_m256i \_mm256\_shufflehi\_epi16(\_\_m256i a, const int n)

.fi
.RE

.SH FLAGS AFFECTED
.PP
None.

.SH SIMD FLOATING\-POINT EXCEPTIONS
.PP
None.

.SH OTHER EXCEPTIONS
.PP
Non\-EVEX\-encoded instruction, see Exceptions Type 4;

.PP
EVEX\-encoded instruction, see Exceptions Type E4NF.nb

.TS
allbox;
l l 
l l .
#UD	T{
If VEX.vvvv != 1111B, or EVEX.vvvv != 1111B.
T}
.TE

.SH SEE ALSO
.PP
x86\-manpages(7) for a list of other x86\-64 man pages.

.SH COLOPHON
.PP
This UNOFFICIAL, mechanically\-separated, non\-verified reference is
provided for convenience, but it may be incomplete or broken in
various obvious or non\-obvious ways. Refer to Intel® 64 and IA\-32
Architectures Software Developer’s Manual for anything serious.

.br
This page is generated by scripts; therefore may contain visual or semantical bugs. Please report them (or better, fix them) on https://github.com/ttmo-O/x86-manpages.

.br
MIT licensed by TTMO 2020 (Turkish Unofficial Chamber of Reverse Engineers - https://ttmo.re).
