.nh
.TH "X86-PHADDW-PHADDD" "7" "May 2019" "TTMO" "Intel x86-64 ISA Manual"
.SH NAME
PHADDW-PHADDD - PACKED HORIZONTAL ADD
.TS
allbox;
l l l l l 
l l l l l .
\fB\fCOpcode/Instruction\fR	\fB\fCOp/En\fR	\fB\fC64/32 bit Mode Support\fR	\fB\fCCPUID Feature Flag\fR	\fB\fCDescription\fR
T{
NP 0F 38 01 /r1 PHADDW mm1, mm2/m64
T}
	RM	V/V	SSSE3	T{
Add 16\-bit integers horizontally, pack to mm1.
T}
T{
66 0F 38 01 /r PHADDW xmm1, xmm2/m128
T}
	RM	V/V	SSSE3	T{
Add 16\-bit integers horizontally, pack to xmm1.
T}
T{
NP 0F 38 02 /r PHADDD mm1, mm2/m64
T}
	RM	V/V	SSSE3	T{
Add 32\-bit integers horizontally, pack to mm1.
T}
T{
66 0F 38 02 /r PHADDD xmm1, xmm2/m128
T}
	RM	V/V	SSSE3	T{
Add 32\-bit integers horizontally, pack to xmm1.
T}
T{
VEX.128.66.0F38.WIG 01 /r VPHADDW xmm1, xmm2, xmm3/m128
T}
	RVM	V/V	AVX	T{
Add 16\-bit integers horizontally, pack to xmm1.
T}
T{
VEX.128.66.0F38.WIG 02 /r VPHADDD xmm1, xmm2, xmm3/m128
T}
	RVM	V/V	AVX	T{
Add 32\-bit integers horizontally, pack to xmm1.
T}
T{
VEX.256.66.0F38.WIG 01 /r VPHADDW ymm1, ymm2, ymm3/m256
T}
	RVM	V/V	AVX2	T{
Add 16\-bit signed integers horizontally, pack to ymm1.
T}
T{
VEX.256.66.0F38.WIG 02 /r VPHADDD ymm1, ymm2, ymm3/m256
T}
	RVM	V/V	AVX2	T{
Add 32\-bit signed integers horizontally, pack to ymm1.
T}
.TE

.PP
.RS

.PP
1\&. See note in Section 2.4, “AVX and SSE Instruction Exception
Specification” in the Intel® 64 and IA\-32 Architectures Software
Developer’s Manual, Volume 3A.

.RE

.SH INSTRUCTION OPERAND ENCODING
.TS
allbox;
l l l l l 
l l l l l .
Op/En	Operand 1	Operand 2	Operand 3	Operand 4
RM	ModRM:reg (r, w)	ModRM:r/m (r)	NA	NA
RVM	ModRM:reg (w)	VEX.vvvv (r)	ModRM:r/m (r)	NA
.TE

.SH DESCRIPTION
.PP
(V)PHADDW adds two adjacent 16\-bit signed integers horizontally from the
source and destination operands and packs the 16\-bit signed results to
the destination operand (first operand). (V)PHADDD adds two adjacent
32\-bit signed integers horizontally from the source and destination
operands and packs the 32\-bit signed results to the destination operand
(first operand). When the source operand is a 128\-bit memory operand,
the operand must be aligned on a 16\-byte boundary or a
general\-protection exception (#GP) will be generated.

.PP
Note that these instructions can operate on either unsigned or signed
(two’s complement notation) integers; however, it does not set bits in
the EFLAGS register to indicate overflow and/or a carry. To prevent
undetected overflow conditions, software must control the ranges of the
values operated on.

.PP
Legacy SSE instructions: Both operands can be MMX registers. The second
source operand can be an MMX register or a 64\-bit memory location.

.PP
128\-bit Legacy SSE version: The first source and destination operands
are XMM registers. The second source operand can be an XMM register or a
128\-bit memory location. Bits (MAXVL\-1:128) of the corresponding YMM
destination register remain unchanged.

.PP
In 64\-bit mode, use the REX prefix to access additional registers.

.PP
VEX.128 encoded version: The first source and destination operands are
XMM registers. The second source operand can be an XMM register or a
128\-bit memory location. Bits (MAXVL\-1:128) of the corresponding YMM
register are zeroed.

.PP
VEX.256 encoded version: Horizontal addition of two adjacent data
elements of the low 16\-bytes of the first and second source operands are
packed into the low 16\-bytes of the destination operand. Horizontal
addition of two adjacent data elements of the high 16\-bytes of the first
and second source operands are packed into the high 16\-bytes of the
destination operand. The first source and destination operands are YMM
registers. The second source operand can be an YMM register or a 256\-bit
memory location.

.PP
Note: VEX.L must be 0, otherwise the instruction will #UD.

.PP
X7X6X5X4X3X2X1X0Y7Y6Y5Y4Y3Y2Y1SRC2SRC1Y0

.PP
S7 S3 S3 S4 S3 S2 S1 S0

.PP
250Dest

.PP
Figure 4\-10. 256\-bit VPHADDD Instruction Operation

.SH OPERATION
.SS PHADDW (with 64\-bit operands)
.PP
.RS

.nf
mm1[15\-0] = mm1[31\-16] + mm1[15\-0];
mm1[31\-16] = mm1[63\-48] + mm1[47\-32];
mm1[47\-32] = mm2/m64[31\-16] + mm2/m64[15\-0];
mm1[63\-48] = mm2/m64[63\-48] + mm2/m64[47\-32];

.fi
.RE

.SS PHADDW (with 128\-bit operands)
.PP
.RS

.nf
xmm1[15\-0] = xmm1[31\-16] + xmm1[15\-0];
xmm1[31\-16] = xmm1[63\-48] + xmm1[47\-32];
xmm1[47\-32] = xmm1[95\-80] + xmm1[79\-64];
xmm1[63\-48] = xmm1[127\-112] + xmm1[111\-96];
xmm1[79\-64] = xmm2/m128[31\-16] + xmm2/m128[15\-0];
xmm1[95\-80] = xmm2/m128[63\-48] + xmm2/m128[47\-32];
xmm1[111\-96] = xmm2/m128[95\-80] + xmm2/m128[79\-64];
xmm1[127\-112] = xmm2/m128[127\-112] + xmm2/m128[111\-96];

.fi
.RE

.SS VPHADDW (VEX.128 encoded version)
.PP
.RS

.nf
DEST[15:0]←SRC1[31:16] + SRC1[15:0]
DEST[31:16]←SRC1[63:48] + SRC1[47:32]
DEST[47:32]←SRC1[95:80] + SRC1[79:64]
DEST[63:48]←SRC1[127:112] + SRC1[111:96]
DEST[79:64]←SRC2[31:16] + SRC2[15:0]
DEST[95:80]←SRC2[63:48] + SRC2[47:32]
DEST[111:96]←SRC2[95:80] + SRC2[79:64]
DEST[127:112]←SRC2[127:112] + SRC2[111:96]
DEST[MAXVL\-1:128] ← 0

.fi
.RE

.SS VPHADDW (VEX.256 encoded version)
.PP
.RS

.nf
DEST[15:0]←SRC1[31:16] + SRC1[15:0]
DEST[31:16]←SRC1[63:48] + SRC1[47:32]
DEST[47:32]←SRC1[95:80] + SRC1[79:64]
DEST[63:48]←SRC1[127:112] + SRC1[111:96]
DEST[79:64]←SRC2[31:16] + SRC2[15:0]
DEST[95:80]←SRC2[63:48] + SRC2[47:32]
DEST[111:96]←SRC2[95:80] + SRC2[79:64]
DEST[127:112]←SRC2[127:112] + SRC2[111:96]
DEST[143:128]←SRC1[159:144] + SRC1[143:128]
DEST[159:144]←SRC1[191:176] + SRC1[175:160]
DEST[175:160]←SRC1[223:208] + SRC1[207:192]
DEST[191:176]←SRC1[255:240] + SRC1[239:224]
DEST[207:192]←SRC2[127:112] + SRC2[143:128]
DEST[223:208]←SRC2[159:144] + SRC2[175:160]
DEST[239:224]←SRC2[191:176] + SRC2[207:192]
DEST[255:240]←SRC2[223:208] + SRC2[239:224]

.fi
.RE

.SS PHADDD (with 64\-bit operands)
.PP
.RS

.nf
mm1[31\-0] = mm1[63\-32] + mm1[31\-0];
mm1[63\-32] = mm2/m64[63\-32] + mm2/m64[31\-0];

.fi
.RE

.SS PHADDD (with 128\-bit operands)
.PP
.RS

.nf
xmm1[31\-0] = xmm1[63\-32] + xmm1[31\-0];
xmm1[63\-32] = xmm1[127\-96] + xmm1[95\-64];
xmm1[95\-64] = xmm2/m128[63\-32] + xmm2/m128[31\-0];
xmm1[127\-96] = xmm2/m128[127\-96] + xmm2/m128[95\-64];

.fi
.RE

.SS VPHADDD (VEX.128 encoded version)
.PP
.RS

.nf
DEST[31\-0]←SRC1[63\-32] + SRC1[31\-0]
DEST[63\-32]←SRC1[127\-96] + SRC1[95\-64]
DEST[95\-64]←SRC2[63\-32] + SRC2[31\-0]
DEST[127\-96]←SRC2[127\-96] + SRC2[95\-64]
DEST[MAXVL\-1:128] ← 0

.fi
.RE

.SS VPHADDD (VEX.256 encoded version)
.PP
.RS

.nf
DEST[31\-0]←SRC1[63\-32] + SRC1[31\-0]
DEST[63\-32]←SRC1[127\-96] + SRC1[95\-64]
DEST[95\-64]←SRC2[63\-32] + SRC2[31\-0]
DEST[127\-96]←SRC2[127\-96] + SRC2[95\-64]
DEST[159\-128]←SRC1[191\-160] + SRC1[159\-128]
DEST[191\-160]←SRC1[255\-224] + SRC1[223\-192]
DEST[223\-192]←SRC2[191\-160] + SRC2[159\-128]
DEST[255\-224]←SRC2[255\-224] + SRC2[223\-192]

.fi
.RE

.SH INTEL C/C++ COMPILER INTRINSIC EQUIVALENTS
.PP
.RS

.nf
PHADDW: \_\_m64 \_mm\_hadd\_pi16 (\_\_m64 a, \_\_m64 b)

PHADDD: \_\_m64 \_mm\_hadd\_pi32 (\_\_m64 a, \_\_m64 b)

(V)PHADDW: \_\_m128i \_mm\_hadd\_epi16 (\_\_m128i a, \_\_m128i b)

(V)PHADDD: \_\_m128i \_mm\_hadd\_epi32 (\_\_m128i a, \_\_m128i b)

VPHADDW: \_\_m256i \_mm256\_hadd\_epi16 (\_\_m256i a, \_\_m256i b)

VPHADDD: \_\_m256i \_mm256\_hadd\_epi32 (\_\_m256i a, \_\_m256i b)

.fi
.RE

.SH SIMD FLOATING\-POINT EXCEPTIONS
.PP
None.

.SH OTHER EXCEPTIONS
.PP
See Exceptions Type 4; additionally

.TS
allbox;
l l 
l l .
#UD	If VEX.L = 1.
.TE

.SH SEE ALSO
.PP
x86\-manpages(7) for a list of other x86\-64 man pages.

.SH COLOPHON
.PP
This UNOFFICIAL, mechanically\-separated, non\-verified reference is
provided for convenience, but it may be incomplete or broken in
various obvious or non\-obvious ways. Refer to Intel® 64 and IA\-32
Architectures Software Developer’s Manual for anything serious.

.br
This page is generated by scripts; therefore may contain visual or semantical bugs. Please report them (or better, fix them) on https://github.com/ttmo-O/x86-manpages.

.br
MIT licensed by TTMO 2020 (Turkish Unofficial Chamber of Reverse Engineers - https://ttmo.re).
