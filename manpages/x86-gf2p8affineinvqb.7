.nh
.TH "X86-GF2P8AFFINEINVQB" "7" "May 2019" "TTMO" "Intel x86-64 ISA Manual"
.SH NAME
GF2P8AFFINEINVQB - GALOIS FIELD AFFINE TRANSFORMATION INVERSE
.TS
allbox;
l l l l l 
l l l l l .
\fB\fCOpcode/Instruction\fR	\fB\fCOp/En\fR	\fB\fC64/32 bit Mode Support\fR	\fB\fCCPUID Feature Flag\fR	\fB\fCDescription\fR
T{
66 0F3A CF /r /ib GF2P8AFFINEINVQB xmm1, xmm2/m128, imm8
T}
	A	V/V	GFNI	T{
Computes inverse affine transformation in the finite field GF(2^8).
T}
.TE

.SH INSTRUCTION OPERAND ENCODING
.TS
allbox;
l l l l l l 
l l l l l l .
\fB\fCOp/En\fR	\fB\fCTuple\fR	\fB\fCOperand 1\fR	\fB\fCOperand 2\fR	\fB\fCOperand 3\fR	\fB\fCOperand 4\fR
A	NA	ModRM:reg (r, w)	ModRM:r/m (r)	imm8 (r)	NA
.TE

.SS Description
.PP
The AFFINEINVB instruction computes an affine transformation in the
Galois Field 23 + x + 1.

.PP
One SIMD register (operand 1) holds “x” as either 16, 32 or 64 8\-bit
vectors. A second SIMD (operand 2) register or memory operand contains
2, 4, or 8 “A” values, which are operated upon by the correspondingly
aligned 8 “x” values in the first register. The “b” vector is constant
for all calculations and contained in the immediate byte.

.PP
The SSE encoded forms of the instruction require 16B alignment on their
memory operations.

.PP
The inverse of each byte is given by the following table. The upper
nibble is on the vertical axis and the lower nibble is on the horizontal
axis. For example, the inverse of 0x95 is 0x8A.

.TS
allbox;
l l l l l l l l l l l l l l l l l 
l l l l l l l l l l l l l l l l l .
\-	0	1	2	3	4	5	6	7	8	9	A	B	C	D	E	F
0	0	1	8D	F6	CB	52	7B	D1	E8	4F	29	C0	B0	E1	E5	C7
1	74	B4	AA	4B	99	2B	60	5F	58	3F	FD	CC	FF	40	EE	B2
2	3A	6E	5A	F1	55	4D	A8	C9	C1	A	98	15	30	44	A2	C2
3	2C	45	92	6C	F3	39	66	42	F2	35	20	6F	77	BB	59	19
4	1D	FE	37	67	2D	31	F5	69	A7	64	AB	13	54	25	E9	9
5	ED	5C	5	CA	4C	24	87	BF	18	3E	22	F0	51	EC	61	17
6	16	5E	AF	D3	49	A6	36	43	F4	47	91	DF	33	93	21	3B
7	79	B7	97	85	10	B5	BA	3C	B6	70	D0	6	A1	FA	81	82
8	83	7E	7F	80	96	73	BE	56	9B	9E	95	D9	F7	2	B9	A4
9	DE	6A	32	6D	D8	8A	84	72	2A	14	9F	88	F9	DC	89	9A
A	FB	7C	2E	C3	8F	B8	65	48	26	C8	12	4A	CE	E7	D2	62
B	C	E0	1F	EF	11	75	78	71	A5	8E	76	3D	BD	BC	86	57
C	B	28	2F	A3	DA	D4	E4	F	A9	27	53	4	1B	FC	AC	E6
D	7A	7	AE	63	C5	DB	E2	EA	94	8B	C4	D5	9D	F8	90	6B
E	B1	D	D6	EB	C6	E	CF	AD	8	4E	D7	E3	5D	50	1E	B3
F	5B	23	38	34	68	46	3	8C	DD	9C	7D	A0	CD	1A	41	1C
.TE

.PP
Table 3\-50. Inverse Byte Listings

.SS Operation
.PP
.RS

.nf
define affine\_inverse\_byte(tsrc2qw, src1byte, imm):
    FOR i ← 0 to 7:
        * parity(x) = 1 if x has an odd number of 1s in it, and 0 otherwise.*
        * inverse(x) is defined in the table above *
        retbyte.bit[i] ← parity(tsrc2qw.byte[7\-i] AND inverse(src1byte)) XOR imm8.bit[i]
    return retbyte

.fi
.RE

.SS GF2P8AFFINEINVQB srcdest, src1, imm8 (128b SSE encoded version)
.PP
.RS

.nf
FOR j ← 0 TO 1:
    FOR b ← 0 to 7:
        SRCDEST.qword[j].byte[b] ← affine\_inverse\_byte(SRC1.qword[j], SRCDEST.qword[j].byte[b], imm8)

.fi
.RE

.SS Intel C/C++ Compiler Intrinsic Equivalent
.PP
.RS

.nf
GF2P8AFFINEINVQB \_\_m128i \_mm\_gf2p8affineinv\_epi64\_epi8(\_\_m128i, \_\_m128i, int);

GF2P8AFFINEINVQB \_\_m128i \_mm\_mask\_gf2p8affineinv\_epi64\_epi8(\_\_m128i, \_\_mmask16, \_\_m128i, \_\_m128i, int);

GF2P8AFFINEINVQB \_\_m128i \_mm\_maskz\_gf2p8affineinv\_epi64\_epi8(\_\_mmask16, \_\_m128i, \_\_m128i, int);

.fi
.RE

.SS SIMD Floating\-Point Exceptions
.PP
None.

.SS Other Exceptions
.PP
Legacy\-encoded: Exceptions Type 4.

.SH SEE ALSO
.PP
x86\-manpages(7) for a list of other x86\-64 man pages.

.SH COLOPHON
.PP
This UNOFFICIAL, mechanically\-separated, non\-verified reference is
provided for convenience, but it may be incomplete or broken in
various obvious or non\-obvious ways. Refer to Intel® 64 and IA\-32
Architectures Software Developer’s Manual for anything serious.

.br
This page is generated by scripts; therefore may contain visual or semantical bugs. Please report them (or better, fix them) on https://github.com/ttmo-O/x86-manpages.

.br
MIT licensed by TTMO 2020 (Turkish Unofficial Chamber of Reverse Engineers - https://ttmo.re).
